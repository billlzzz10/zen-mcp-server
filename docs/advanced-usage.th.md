# คู่มือการใช้งานขั้นสูง

เอกสารนี้อธิบายฟีเจอร์ขั้นสูง ตัวเลือกการตั้งค่า และเวิร์กโฟลว์สำหรับผู้ใช้ Zen MCP Server ที่ต้องการพลังเต็มรูปแบบ

## สารบัญ

- [การตั้งค่าโมเดล](#การตั้งค่าโมเดล)
- [ข้อจำกัดการใช้งานโมเดล](#ข้อจำกัดการใช้งานโมเดล)
- [Thinking Mode](#thinking-mode)
- [พารามิเตอร์ของเครื่องมือ](#พารามิเตอร์ของเครื่องมือ)
- [Context Revival: ความทรงจำที่เกินขีดจำกัดบริบท](#context-revival-ความทรงจำที่เกินขีดจำกัดบริบท)
- [เวิร์กโฟลว์การร่วมมือ](#เวิร์กโฟลว์การร่วมมือ)
- [การทำงานกับพรอมต์ขนาดใหญ่](#การทำงานกับพรอมต์ขนาดใหญ่)
- [การรองรับ Vision](#การรองรับ-vision)
- [การผนวกการค้นเว็บ](#การผนวกการค้นเว็บ)
- [System Prompt](#system-prompt)

## การตั้งค่าโมเดล

หากต้องการตั้งค่าง่าย ๆ ดู [Configuration Guide](configuration.md) ซึ่งอธิบายการใส่ API key การเลือกโมเดล และตัวแปรสภาพแวดล้อม

ส่วนนี้เน้นรูปแบบการใช้โมเดลขั้นสูงสำหรับ power user

**สั่งเปลี่ยนโมเดลเป็นรายคำขอ:** ไม่ว่า `DEFAULT_MODEL` จะตั้งไว้แบบใด คุณสามารถระบุโมเดลในคำสั่งได้โดยตรง เช่น
- "ใช้ **pro** วิเคราะห์ความปลอดภัยของ auth.py แบบเจาะลึก"
- "ใช้ **flash** เพื่อจัดรูปโค้ดให้เร็ว"
- "ใช้ **o3** ช่วยดีบักตรรกะ"
- "รีวิวด้วย **o4-mini** เพื่อสมดุลความเร็วและคุณภาพ"
- "ใช้ **gpt4.1** วิเคราะห์โค้ดเบสทั้งชุด"

### ตารางตัดสินใจ Auto Mode ของ Claude

| โมเดล | ผู้ให้บริการ | ขนาดบริบท | จุดเด่น | Auto Mode ใช้เมื่อ |
|-------|---------------|------------|---------|---------------------|
| **`pro`** (Gemini 2.5 Pro) | Google | 1M โทเคน | คิดเชิงลึก (ได้ถึง 32K โทเคน), วิเคราะห์ซับซ้อน | สถาปัตยกรรมใหญ่ ความปลอดภัย ดีบักเชิงลึก |
| **`flash`** (Gemini 2.5 Flash) | Google | 1M โทเคน | ตอบไวมากพร้อม thinking | ตรวจเร็ว ๆ จัดรูปแบบ วิเคราะห์เบื้องต้น |
| **`flash-2.0`** (Gemini 2.0 Flash) | Google | 1M โทเคน | โมเดลเร็วรุ่นล่าสุด รองรับเสียง/วิดีโอ | วิเคราะห์ไวพร้อมอินพุตมัลติมีเดีย |
| **`flashlite`** (Gemini 2.0 Flash Lite) | Google | 1M โทเคน | โมเดลน้ำหนักเบา (ข้อความล้วน) | ประมวลผลข้อความเร็วกว่าด้วยต้นทุนต่ำ |
| **`o3`** | OpenAI | 200K โทเคน | เหตุผลเชิงตรรกะยอดเยี่ยม | ดีบักตรรกะ วิเคราะห์เป็นระบบ |
| **`o3-mini`** | OpenAI | 200K โทเคน | สมดุลความเร็ว/คุณภาพ | งานระดับกลาง |
| **`o4-mini`** | OpenAI | 200K โทเคน | โมเดลเหตุผลรุ่นใหม่ | งานบริบทสั้นต้องการคุณภาพ |
| **`gpt4.1`** | OpenAI | 1M โทเคน | GPT-4 รุ่นล่าสุดบริบทยาว | วิเคราะห์โค้ดเบสขนาดใหญ่ |
| **`gpt5`** | OpenAI | 400K โทเคน | GPT-5 เต็มรูปแบบ พร้อม reasoning | ปัญหาซับซ้อนต้องการเหตุผลขั้นสูง |
| **`gpt5-mini`** | OpenAI | 400K โทเคน | รุ่นย่อสมดุล | งานทั่วไปที่ต้องการความสามารถพร้อมประหยัด |
| **`gpt5-nano`** | OpenAI | 400K โทเคน | เร็วและถูกที่สุดในตระกูล GPT-5 | สรุปผล/จัดหมวดหมู่ |
| **`grok-4`** | X.AI | 256K โทเคน | โมเดล Grok ธงใหญ่ เหตุผล+วิชั่น | วิเคราะห์เชิงลึกที่มีภาพประกอบ |
| **`grok-3`** | X.AI | 131K โทเคน | โมเดลเหตุผลขั้นสูง | ปัญหาซับซ้อน |
| **`grok-3-fast`** | X.AI | 131K โทเคน | เน้นความเร็ว | ตอบไวพร้อมเหตุผล |
| **`llama`** (Llama 3.2) | Custom/Local | 128K โทเคน | รันในเครื่อง มีความเป็นส่วนตัว | วิเคราะห์ในอุปกรณ์เอง ต้นทุนศูนย์ |
| **โมเดลใดก็ได้** | OpenRouter | ไม่แน่นอน | เข้าถึง GPT-4, Claude, Llama ฯลฯ | ระบุเองหรือให้ระบบเลือกตามงาน |

**ผสมหลายผู้ให้บริการ:** ตั้งทั้ง `OPENROUTER_API_KEY` และ `CUSTOM_API_URL` เพื่อใช้โมเดลคลาวด์ (ทรงพลังแต่มีค่าใช้จ่าย) ควบคู่โมเดลโลคอล (ฟรี/ส่วนตัว) ในบทสนทนาเดียว

**ความสามารถสำคัญของโมเดล:**
- **Gemini** – รองรับ thinking mode (minimal ถึง max), ค้นเว็บ, บริบท 1M
  - **Pro 2.5**: วิเคราะห์ลึก thinking สูงสุด 32K
  - **Flash 2.5**: เร็วมากพร้อม thinking (24K)
  - **Flash 2.0**: รุ่นเร็วล่าสุด รองรับเสียง/วิดีโอ (24K)
  - **Flash Lite 2.0**: ข้อความล้วน น้ำหนักเบา (ไม่มี thinking)
- **O3/O4** – เหตุผลยอดเยี่ยม บริบท 200K
- **GPT-4.1** – บริบทยาว 1M ความสามารถทั่วไป
- **ตระกูล GPT-5** – เหตุผลขั้นสูง บริบท 400K หลายรุ่นย่อย
- **Grok-4/3** – รองรับวิชั่น คิดเชิงลึก บริบท 256K/131K

## ข้อจำกัดการใช้งานโมเดล

รายละเอียดเต็มดูที่ [Configuration Guide](configuration.md#model-usage-restrictions)

### ยุทธศาสตร์จำกัดการใช้ (ตัวอย่าง)
```env
# โหมดพัฒนา เปิดให้ลองหลายโมเดล
GOOGLE_ALLOWED_MODELS=flash,pro
OPENAI_ALLOWED_MODELS=o4-mini,o3-mini

# โหมดโปรดักชัน เน้นประหยัด
GOOGLE_ALLOWED_MODELS=flash
OPENAI_ALLOWED_MODELS=o4-mini

# โหมดสมรรถนะสูง
GOOGLE_ALLOWED_MODELS=pro
OPENAI_ALLOWED_MODELS=gpt-5,o3
```

ตั้งรายการ `*_ALLOWED_MODELS` เพื่อบังคับโมเดลที่ใช้ได้กับแต่ละผู้ให้บริการ Auto mode ก็จะเคารพข้อจำกัดนี้ด้วย

## Thinking Mode

Thinking mode ช่วยกำหนดระดับการให้เหตุผลและจำนวนโทเคน "คิด" ก่อนตอบ (รองรับโดย Gemini และโมเดลเหตุผลบางตัว)

| ค่าพารามิเตอร์ | คำอธิบาย |
|----------------|-----------|
| `minimal` | เน้นความเร็ว ตอบสั้น ไม่วิเคราะห์ลึก |
| `low` | สมดุลความเร็วกับการไตร่ตรอง |
| `medium` | คิดลึกมากขึ้น เหมาะกับงานทั่วไป |
| `high` | วิเคราะห์ละเอียด ใช้โทเคนคิดจำนวนมาก |
| `max` | ใช้ขีดสุดของโทเคนคิด (เช่น Gemini Pro 32K) |

ตั้งค่าได้ผ่าน `DEFAULT_THINKING_MODE`, `DEFAULT_THINKING_MODE_CHAT`, `DEFAULT_THINKING_MODE_THINKDEEP` หรือเลือกต่อคำสั่ง เช่น `"thinkdeep high"`

## พารามิเตอร์ของเครื่องมือ

เครื่องมือแต่ละตัวรองรับพารามิเตอร์เพิ่มเติม เช่น
- `chat`: `model`, `temperature`, `thinking_mode`, `max_output_tokens`
- `thinkdeep`: `model`, `thinking_mode`, `depth`
- `planner`: `priority`, `max_steps`
- `codereview`: `severity_filter`, `include_positive`, `max_issues`

ดูรายละเอียดพารามิเตอร์เฉพาะของแต่ละเครื่องมือได้ในไฟล์ภายใต้ `docs/tools/`

## Context Revival: ความทรงจำที่เกินขีดจำกัดบริบท

Context revival คือความสามารถที่ให้โมเดลอื่น ๆ ช่วยรื้อฟื้นบทสนทนาเมื่อเครื่องมือหลัก (เช่น Claude) ถูกรีเซ็ตบริบท

### วิธีทำงานโดยย่อ
1. Claude เรียกใช้เครื่องมือ (เช่น `codereview`) และแบ่งปันบริบทให้โมเดลอื่น เช่น Gemini หรือ O3
2. โมเดลเหล่านั้นเก็บสรุปหรือคำตอบไว้ในฐานข้อมูลบทสนทนาเบื้องหลัง
3. หาก Claude สูญเสียบริบท ให้สั่ง "continue with <model>" โมเดลนั้นจะส่งข้อมูลซ้ำเพื่อให้ Claude จำได้
4. บริบทเวิร์กโฟลว์จึงรักษาความต่อเนื่อง แม้จะข้ามเครื่องมือไปกลับหลายรอบ

คุณสามารถปรับค่าจำกัดเวลาและจำนวนรอบได้ในไฟล์ `.env` ผ่าน `CONVERSATION_TIMEOUT_HOURS` และ `MAX_CONVERSATION_TURNS`

## เวิร์กโฟลว์การร่วมมือ

Zen MCP ทำให้งานซับซ้อนกลายเป็นเวิร์กโฟลว์ที่หลายโมเดลทำงานร่วมกันได้:

- **ดีเบตเพื่อหาฉันทามติ** – ใช้ `consensus` ให้โมเดลหลายตัวถกเถียงและสรุปเห็นตรง/ต่าง
- **วางแผนแล้วลงมือ** – ใช้ `planner` แยกงาน → มอบหมายให้ `chat` หรือ `refactor` ลงมือ → ยืนยันด้วย `precommit`
- **รีวิวหลายมุมมอง** – ให้ Claude, Gemini, O3 วิเคราะห์โค้ดคนละรอบ ค่อยรวมผลเป็นรายการเดียว
- **หมุนเวียนบทบาท** – ใช้ `clink` เปิด CLI ภายนอกให้จำลองบทบาทพิเศษ เช่น นักทดสอบ นักวิเคราะห์ความปลอดภัย

## การทำงานกับพรอมต์ขนาดใหญ่

โปรโตคอล MCP จำกัดคำขอ/คำตอบรวมประมาณ 25K โทเคน Zen MCP จึงมีระบบอัปโหลดไฟล์อัตโนมัติเมื่อพรอมต์ใหญ่เกินกำหนด (ค่าเริ่มต้น 50K ตัวอักษร)

### ลำดับการทำงาน
1. เซิร์ฟเวอร์ตรวจพบว่าพรอมต์ใหญ่เกิน → แจ้งให้เซฟเป็นไฟล์ `prompt.txt`
2. Claude บันทึกข้อความลงไฟล์ชั่วคราวแล้วส่งคำขอใหม่โดยอ้าง path
3. เซิร์ฟเวอร์อ่านไฟล์โดยตรงและส่งต่อให้โมเดลที่บริบทใหญ่ (เช่น Gemini 1M โทเคน)
4. พื้นที่โทเคนของ MCP จึงเก็บไว้ใช้สำหรับคำตอบได้เต็มที่

ด้วยกลไกนี้ คุณสามารถส่งคำอธิบายยาวมากให้โมเดลได้โดยไม่ชนเพดานของโปรโตคอล MCP

## การรองรับ Vision

เมื่อใช้โมเดลที่รองรับ (เช่น Gemini Pro, GPT-5, Grok-4)
- สามารถแนบไฟล์ภาพ (PNG/JPEG) ผ่านระบบไฟล์ของ MCP client
- เครื่องมืออย่าง `analyze`, `debug`, `docgen` จะรวมภาพเป็นบริบทได้หากโมเดลรองรับ vision
- ให้ระวังขนาดไฟล์ภาพ เพราะจะกินพื้นที่บริบทเหมือนข้อความ

## การผนวกการค้นเว็บ

Web search เปิดไว้เป็นค่าเริ่มต้น โมเดล (โดยเฉพาะ Gemini) จะวิเคราะห์และเสนอรายการค้นที่ควรให้ Claude รันเพิ่มเติมเพื่อยืนยันข้อมูลล่าสุด

### วงจรทำงาน
1. โมเดลวิเคราะห์คำขอและระบุหัวข้อที่ต้องหาข้อมูลอัปเดต
2. ตอบตามความรู้ปัจจุบัน
3. เพิ่มหัวข้อ "Recommended Web Searches for Claude" เป็นรายการค้นเฉพาะ
4. Claude สามารถรันคำค้นเหล่านี้และเสริมคำตอบได้

**ประโยชน์**
- เข้าถึงคู่มือ/API ปัจจุบันเสมอ
- โมเดลมุ่งไปที่การให้เหตุผล ส่วน Claude รับหน้าที่ค้นจริง
- ลดการเดา เพราะสนับสนุนให้ตรวจสอบข้อมูลเพิ่มเติม

หากต้องการปิด web search ให้ตั้งค่าที่ไคลเอนต์หรือปิดเครื่องมือค้นเว็บใน CLI ที่ใช้งาน

## System Prompt

เซิร์ฟเวอร์ใช้ system prompt ที่ออกแบบเฉพาะสำหรับแต่ละเครื่องมือ

### สถาปัตยกรรม prompt
- **ศูนย์กลาง**: prompt แต่ละตัวอยู่ใน `systemprompts/` (เช่น `systemprompts/chat_prompt.py`)
- **การผูกกับเครื่องมือ**: คลาสเครื่องมือสืบทอด `BaseTool` และ override `get_system_prompt()`
- **ลำดับการทำงาน**: คำขอผู้ใช้ → เลือกเครื่องมือ → system prompt + บริบท → คำตอบจากโมเดล

### ความเชี่ยวชาญเฉพาะทาง
- `thinkdeep` – พาร์ตเนอร์อาวุโส ตรวจสอบสมมติฐาน หา edge case
- `codereview` – ผู้รีวิวโค้ดระดับมืออาชีพ ให้ระดับความรุนแรงและคำแนะนำ actionable
- `debug` – นักดีบักที่วิเคราะห์ root cause และแนวทางป้องกัน
- `analyze` – นักวิเคราะห์สถาปัตย์โค้ด มองหาแพทเทิร์นและ insight ที่ทำได้จริง

### การปรับแต่ง
1. แก้ prompt ใน `systemprompts/` เพื่อเปลี่ยนทั่วทั้งระบบ
2. override `get_system_prompt()` ในเครื่องมือใดเครื่องมือหนึ่งเพื่อเปลี่ยนเฉพาะจุด
3. ปรับ `temperature` เพื่อกำหนดสไตล์คำตอบ (เช่น 0.2 สำหรับโฟกัส 0.7 สำหรับคิดสร้างสรรค์)
